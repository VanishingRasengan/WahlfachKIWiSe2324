{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPLiFgl7+bjdd+79WsHcNGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanishingRasengan/WahlfachKIWiSe2324/blob/main/Lektion_3_L%C3%B6sungen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Regression Trees\n",
        "Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression. In this chapter, you'll be introduced to the CART algorithm."
      ],
      "metadata": {
        "id": "6GYqdpgiVh99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your first classification tree\n",
        "In this exercise you'll work with the Wisconsin Breast Cancer Dataset from the UCI machine learning repository. You'll predict whether a tumor is malignant or benign based on two features: the mean radius of the tumor (radius_mean) and its mean number of concave points (concave points_mean)."
      ],
      "metadata": {
        "id": "hKwFJbt5OvQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os, scipy, sklearn\n",
        "import sklearn.metrics, sklearn.preprocessing, sklearn.model_selection, sklearn.tree, sklearn.linear_model, sklearn.cluster\n",
        "!pip install mlxtend --upgrade --no-deps\n",
        "\n",
        "# importing the pathlib module - a handy library for working with the local file system in an object oriented way\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# creating a path object of our data directory within the mounted Google Drive\n",
        "wbc_path = Path('/content/')\n",
        "\n",
        "### to download the .csv file\n",
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://assets.datacamp.com/production/repositories/1796/datasets/0eb6987cb9633e4d6aa6cfd11e00993d2387caa4/wbc.csv')\n",
        "\n",
        "# create another pathlib object with the path to the csv file\n",
        "wbc_csv_path = wbc_path / 'wbc.csv'\n",
        "wbc_df = pd.read_csv(wbc_csv_path)"
      ],
      "metadata": {
        "id": "BIrZofhjBt8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6c3b82-3160-4bc8-a211-8069968ae90a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Collecting mlxtend\n",
            "  Downloading mlxtend-0.23.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mlxtend\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.22.0\n",
            "    Uninstalling mlxtend-0.22.0:\n",
            "      Successfully uninstalled mlxtend-0.22.0\n",
            "Successfully installed mlxtend-0.23.0\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=fb973ebda3484a33ce41bfa5dcfbfa10d6528ad9603576f59f8270cfde818bc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wbc_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "-opTbhX1JeF2",
        "outputId": "6bc20c48-a049-4be4-f560-196e5cba4daa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0      842302         M        17.99         10.38          122.80     1001.0   \n",
              "1      842517         M        20.57         17.77          132.90     1326.0   \n",
              "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3    84348301         M        11.42         20.38           77.58      386.1   \n",
              "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
              "..        ...       ...          ...           ...             ...        ...   \n",
              "564    926424         M        21.56         22.39          142.00     1479.0   \n",
              "565    926682         M        20.13         28.25          131.20     1261.0   \n",
              "566    926954         M        16.60         28.08          108.30      858.1   \n",
              "567    927241         M        20.60         29.33          140.10     1265.0   \n",
              "568     92751         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0    ...          17.33           184.60      2019.0           0.16220   \n",
              "1    ...          23.41           158.80      1956.0           0.12380   \n",
              "2    ...          25.53           152.50      1709.0           0.14440   \n",
              "3    ...          26.50            98.87       567.7           0.20980   \n",
              "4    ...          16.67           152.20      1575.0           0.13740   \n",
              "..   ...            ...              ...         ...               ...   \n",
              "564  ...          26.40           166.10      2027.0           0.14100   \n",
              "565  ...          38.25           155.00      1731.0           0.11660   \n",
              "566  ...          34.12           126.70      1124.0           0.11390   \n",
              "567  ...          39.42           184.60      1821.0           0.16500   \n",
              "568  ...          30.37            59.16       268.6           0.08996   \n",
              "\n",
              "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0              0.66560           0.7119                0.2654          0.4601   \n",
              "1              0.18660           0.2416                0.1860          0.2750   \n",
              "2              0.42450           0.4504                0.2430          0.3613   \n",
              "3              0.86630           0.6869                0.2575          0.6638   \n",
              "4              0.20500           0.4000                0.1625          0.2364   \n",
              "..                 ...              ...                   ...             ...   \n",
              "564            0.21130           0.4107                0.2216          0.2060   \n",
              "565            0.19220           0.3215                0.1628          0.2572   \n",
              "566            0.30940           0.3403                0.1418          0.2218   \n",
              "567            0.86810           0.9387                0.2650          0.4087   \n",
              "568            0.06444           0.0000                0.0000          0.2871   \n",
              "\n",
              "     fractal_dimension_worst  Unnamed: 32  \n",
              "0                    0.11890          NaN  \n",
              "1                    0.08902          NaN  \n",
              "2                    0.08758          NaN  \n",
              "3                    0.17300          NaN  \n",
              "4                    0.07678          NaN  \n",
              "..                       ...          ...  \n",
              "564                  0.07115          NaN  \n",
              "565                  0.06637          NaN  \n",
              "566                  0.07820          NaN  \n",
              "567                  0.12400          NaN  \n",
              "568                  0.07039          NaN  \n",
              "\n",
              "[569 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-adf948fc-e18b-4916-b0ab-01440e23e74e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>...</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>...</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>...</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>...</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adf948fc-e18b-4916-b0ab-01440e23e74e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-adf948fc-e18b-4916-b0ab-01440e23e74e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-adf948fc-e18b-4916-b0ab-01440e23e74e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbc6ab66-82e5-4279-bd84-31c19bab0989\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbc6ab66-82e5-4279-bd84-31c19bab0989')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbc6ab66-82e5-4279-bd84-31c19bab0989 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = wbc_df[['radius_mean', 'concave points_mean']]\n",
        "y = wbc_df['diagnosis']\n",
        "y = y.map({'M':1, 'B':0})"
      ],
      "metadata": {
        "id": "4sGDejdH8_Xk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "mxEYIG_9MjqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is now loaded in your workspace and is split into 80% train and 20% test. The feature matrices are assigned to X_train and X_test, while the arrays of labels are assigned to y_train and y_test where class 1 corresponds to a malignant tumor and class 0 corresponds to a benign tumor. To obtain reproducible results, we also defined a variable called SEED which is set to 1.\n",
        "\n",
        "- Import DecisionTreeClassifier from sklearn.tree.\n",
        "- Instantiate a DecisionTreeClassifier dt of maximum depth equal to 6.\n",
        "- Fit dt to the training set.\n",
        "- Predict the test set labels and assign the result to y_pred."
      ],
      "metadata": {
        "id": "WgQ97T03O7C6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YA_0IH94Mmwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091d0cec-9c58-4bb2-d6e9-c962b9c56c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Import DecisionTreeClassifier from sklearn.tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
        "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = dt.predict(X_test)\n",
        "print(y_pred[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the classification tree\n",
        "Now that you've fit your first classification tree, it's time to evaluate its performance on the test set. You'll do so using the accuracy metric which corresponds to the fraction of correct predictions made on the test set.\n",
        "\n",
        "The trained model dt from the previous exercise is loaded in your workspace along with the test set features matrix X_test and the array of labels y_test.\n",
        "\n",
        "- Import the function accuracy_score from sklearn.metrics.\n",
        "- Predict the test set labels and assign the obtained array to y_pred.\n",
        "- Evaluate the test set accuracy score of dt by calling accuracy_score() and assign the value to acc."
      ],
      "metadata": {
        "id": "kPlk2miUVQeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Compute test set accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: {:.2f}\".format(acc))"
      ],
      "metadata": {
        "id": "ZTfSr7ABN5FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab61e79-d238-4da6-b225-8b97acc0471f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "Finally, we want to have a look at Neural Networks!"
      ],
      "metadata": {
        "id": "QuZWFt_TgNvQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yOkGTUF6Kp"
      },
      "source": [
        " ## Setting up a network model and starting a first training\n",
        "\n",
        "In this and the following exercise, we are going to practise, how to set up a neural network model and perform a first training with this network.\n",
        "We will use the DermaMNIST from the MedMNIST datasets, which you have seen last lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dnthk05cBlAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc8e482-91f1-4bb0-fa0a-e782fd8e5dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-2.2.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.16.0+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=c9ccf4d41e586b6f3a8ff182e1d5731fbec22fb83ad6ab6db90ed93b02c1be1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-2.2.3\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "# from tqdm import trange\n",
        "# from tqdm import tqdm\n",
        "# from skimage.util import montage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision as torchvision\n",
        "\n",
        "!pip install medmnist\n",
        "import medmnist\n",
        "from medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n",
        "from medmnist.evaluator import getAUC, getACC\n",
        "from medmnist.info import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MGpKZ1TFwfnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c1e53f-3a97-4961-e645-6de2db7d8cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 2.2.3\n"
          ]
        }
      ],
      "source": [
        "print(\"Version:\", medmnist.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x6nPfDx8wiAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd69f694-1f62-496a-9487-70ca5507c577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info:\n",
            "{'python_class': 'DermaMNIST', 'description': 'The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.', 'url': 'https://zenodo.org/record/6496656/files/dermamnist.npz?download=1', 'MD5': '0744692d530f8e62ec473284d019b0c7', 'task': 'multi-class', 'label': {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}, 'n_channels': 3, 'n_samples': {'train': 7007, 'val': 1003, 'test': 2005}, 'license': 'CC BY-NC 4.0'}\n",
            "\n",
            "Task:\n",
            "multi-class\n",
            "\n",
            "Channels:\n",
            "3\n",
            "\n",
            "Number of classes:\n",
            "7\n",
            "\n",
            "Label:\n",
            "{'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# various MedMNIST datasets\n",
        "data_flag = 'dermamnist'\n",
        "download = True\n",
        "input_root = 'tmp_data/'\n",
        "!mkdir 'tmp_data'\n",
        "\n",
        "flag_to_class = {\n",
        "    \"pathmnist\": PathMNIST,\n",
        "    \"chestmnist\": ChestMNIST,\n",
        "    \"dermamnist\": DermaMNIST,\n",
        "    \"octmnist\": OCTMNIST,\n",
        "    \"pneumoniamnist\": PneumoniaMNIST,\n",
        "    \"retinamnist\": RetinaMNIST,\n",
        "    \"breastmnist\": BreastMNIST,\n",
        "    \"organmnist_axial\": OrganMNISTAxial,\n",
        "    \"organmnist_coronal\": OrganMNISTCoronal,\n",
        "    \"organmnist_sagittal\": OrganMNISTSagittal,\n",
        "}\n",
        "\n",
        "DataClass = flag_to_class[data_flag]\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "label_dict = info['label']\n",
        "\n",
        "print(f\"Info:\\n{info}\\n\")\n",
        "print(f\"Task:\\n{task}\\n\")\n",
        "print(f\"Channels:\\n{n_channels}\\n\")\n",
        "print(f\"Number of classes:\\n{n_classes}\\n\")\n",
        "print(f\"Label:\\n{label_dict}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2I6F49b8fRx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Defining the augmentations\n",
        "\n",
        "We now define the augmentations, in particular Resizing, Random Rotation and Normalizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HjF-Aq9BF6K_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Imagenet values\n",
        "norm_mean = (0.4914)\n",
        "norm_std = (0.2023)\n",
        "\n",
        "# define the transformaitons the images go through each time it is used for training\n",
        "# includes augmentation AND normalization as described above\n",
        "augmentation_train = transforms.Compose([\n",
        "                                  # resize image to the network input size\n",
        "                                  transforms.Resize((28,28)),\n",
        "                                  # rotate the image with a certain angle range, randomly chosen\n",
        "                                  transforms.RandomRotation(degrees=20),\n",
        "                                  # convert the image into a tensor so it can be processed by the GPU\n",
        "                                  transforms.ToTensor(),\n",
        "                                  # normalize the image with the mean and std of ImageNet\n",
        "                                  transforms.Normalize(norm_mean, norm_std),\n",
        "                                   ])\n",
        "\n",
        "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
        "augmentation_test = transforms.Compose([\n",
        "                    transforms.Resize((28,28)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(norm_mean, norm_std),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGR6pIuMJsqK"
      },
      "source": [
        "## Splitting up data\n",
        "\n",
        "Set up datasets for training, validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AHYTu04_yDvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e47adeb-5cc2-4df2-84a1-9c363d10721a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/6496656/files/dermamnist.npz to tmp_data/dermamnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19725078/19725078 [00:02<00:00, 8291498.85it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n",
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "train_dataset = DataClass(root=input_root, split='train', transform=augmentation_train, download=download)\n",
        "test_dataset = DataClass(root=input_root, split='test', transform=augmentation_test, download=download)\n",
        "val_dataset = DataClass(root=input_root, split='val', transform=augmentation_test, download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zL9p3c52xpUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226554dd-759c-423e-e869-a588c271f302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 7007\n",
            "    Root location: tmp_data/\n",
            "    Split: train\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 1003\n",
            "    Root location: tmp_data/\n",
            "    Split: val\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 2005\n",
            "    Root location: tmp_data/\n",
            "    Split: test\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n"
          ]
        }
      ],
      "source": [
        "# Some detailed information about all splits\n",
        "print(\"===================\")\n",
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(val_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZXPqSvKNyh"
      },
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "PyTorch provides template Dataloader classes for easy data handling, assigning according transforms and splits. You can find more information about how PyTorch handles datasets and data loading [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VUURp9J01EtJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "### encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NMfm-GkQyFe3"
      },
      "outputs": [],
      "source": [
        "### the next() function returns the next item from the iterator.\n",
        "batch_images, batch_labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7O9BTWgOyIhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8559ef9-0448-492c-b10a-7207d1dce7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d635a7af520>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# show all 3 channels of image 10\n",
        "print(batch_images[0].shape)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][0,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][1,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][2,:,:])\n",
        "\n",
        "### different color maps\n",
        "### cmap='bone', cmap = 'summer', cmap = 'seismic'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3QclrDwU6j2B"
      },
      "source": [
        "## Define a Convolutional Neural Network\n",
        "\n",
        "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from [torch library](https://pytorch.org/docs/stable/nn.html).\n",
        "\n",
        "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authors’ implementation of LeNet was used primarily for OCR and character recognition in documents. The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs.\n",
        "\n",
        "To define a neural network in PyTorch one has to create a class inhereting from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Crucially, this class has to include the function forward() which defines which computation should be performed at every call given a batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C4SswIxD6j2B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = 7\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), padding=2)\n",
        "        self.nonlin1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5))\n",
        "        self.nonlin2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.nonlin3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.nonlin4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "        self.nonlin5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.nonlin1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.nonlin2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.nonlin3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.nonlin4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.nonlin5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYDXWn0pBF6"
      },
      "source": [
        "You can use torchsummary to print out the architecture of your model given a certain input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gpD04oeOpBF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5406e2bb-3d71-47ec-cf10-05d41e53250d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [127, 6, 28, 28]             156\n",
            "              ReLU-2           [127, 6, 28, 28]               0\n",
            "         MaxPool2d-3           [127, 6, 14, 14]               0\n",
            "            Conv2d-4          [127, 16, 10, 10]           2,416\n",
            "              ReLU-5          [127, 16, 10, 10]               0\n",
            "         MaxPool2d-6            [127, 16, 5, 5]               0\n",
            "            Linear-7                 [127, 120]          48,120\n",
            "              ReLU-8                 [127, 120]               0\n",
            "            Linear-9                  [127, 84]          10,164\n",
            "             ReLU-10                  [127, 84]               0\n",
            "           Linear-11                   [127, 7]             595\n",
            "             ReLU-12                   [127, 7]               0\n",
            "================================================================\n",
            "Total params: 61,451\n",
            "Trainable params: 61,451\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.38\n",
            "Forward/backward pass size (MB): 14.15\n",
            "Params size (MB): 0.23\n",
            "Estimated Total Size (MB): 14.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = LeNet()\n",
        "model.to(device)\n",
        "\n",
        "summary(model, input_size=(1,28,28), batch_size=127)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf87hhWCaqp"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Create a neural network capable of processing the image tensor 'img', which has only one channel. The network should contain at least one convolutional layer and one additional fully connected layer. Pay attention that within the fully connected layer the output dimension of the last convolutional layer has to fit the input dimension. Additionally, the output dimension of the fully connected layer before 'fc_fin' has to match the required input dimension of 'fc_fin'.\n",
        "\n",
        "Some hints:\n",
        "\n",
        "- Have a look at how LeNet is implemented above. Many concepts can be copied.\n",
        "- Focus on using the PyTorch's Linear, Conv2d, MaxPool2d and ReLU layers and the .view() function. You can find detailed information on these components in [PyTorch's documentation](https://pytorch.org/docs/stable/index.html).\n",
        "- Batch size does not have to be considered when designing the neural networks. PyTorch adapts the calculations automatically when provided with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3ia2S8hrDCIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a123cf4-1f4a-4e1c-efca-090379f0be2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The correct output size has been generated.\n"
          ]
        }
      ],
      "source": [
        "img = torch.rand((1, 1, 200, 200))\n",
        "\n",
        "output_dim = 6\n",
        "\n",
        "class LeNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet2, self).__init__()\n",
        "\n",
        "        # --------------------\n",
        "\n",
        "        ########## Insert your layers here ##########\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.nonlin1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
        "        self.fc1   = nn.Linear(16*48*48, 120)\n",
        "        self.fc2   = nn.Linear(120, 48)\n",
        "        # --------------------\n",
        "\n",
        "        self.fc_fin = nn.Linear(in_features=48, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # --------------------\n",
        "\n",
        "        ########## Insert your forward pass here ##########\n",
        "        x = self.conv1(x)\n",
        "        x = self.nonlin1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # --------------------\n",
        "\n",
        "        x = self.fc_fin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "model = LeNet2()\n",
        "\n",
        "output = model(img)\n",
        "\n",
        "if output.size(1) == 6:\n",
        "    print('The correct output size has been generated.')\n",
        "else:\n",
        "    print('The generated output size is not correct')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = LeNet2()\n",
        "model.to(device)\n",
        "\n",
        "summary(model, input_size=(1,200,200), batch_size=127)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOuocbA5aQGZ",
        "outputId": "02af38ce-5ca7-4224-e8e2-8177f30f33c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [127, 6, 200, 200]             156\n",
            "              ReLU-2         [127, 6, 200, 200]               0\n",
            "         MaxPool2d-3         [127, 6, 100, 100]               0\n",
            "            Conv2d-4          [127, 16, 96, 96]           2,416\n",
            "            Linear-5                 [127, 120]       4,423,800\n",
            "            Linear-6                  [127, 48]           5,808\n",
            "            Linear-7                   [127, 6]             294\n",
            "================================================================\n",
            "Total params: 4,432,474\n",
            "Trainable params: 4,432,474\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 19.38\n",
            "Forward/backward pass size (MB): 666.27\n",
            "Params size (MB): 16.91\n",
            "Estimated Total Size (MB): 702.55\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_R_SSWF6Lb"
      },
      "source": [
        "## Define a Loss function\n",
        "\n",
        "Let's use a Classification Cross-Entropy loss.\n",
        "\n",
        "$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$\n",
        "\n",
        "### Median Frequency Balancing\n",
        "There are datasets which have a large imbalance in the amount of label occurrence. A prediction would be therefore biased towards stronger represented classes. As a solution, we use **Median Frequency Balancing**. Essentially this tasks the optimizer to weight examples of rare cases more highly than examples of common cases that are processed more frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zy8HJZDRvdB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010b57bb-5eda-48ef-ab19-31a7db4be6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 0 : 1.5745614035087718\n",
            "class 1 : 1.0\n",
            "class 2 : 0.4668400520156047\n",
            "class 3 : 4.4875\n",
            "class 4 : 0.46084724005134786\n",
            "class 5 : 0.07649691029192414\n",
            "class 6 : 3.6262626262626263\n"
          ]
        }
      ],
      "source": [
        "# get the class labels of each image\n",
        "class_labels = train_dataset.labels\n",
        "# get the number of different classes\n",
        "num_classes = np.max(class_labels)+1\n",
        "# empty array for counting instance of each class\n",
        "count_labels = np.zeros(num_classes)\n",
        "# empty array for weights of each class\n",
        "class_weights = np.zeros(num_classes)\n",
        "\n",
        "# populate the count array\n",
        "for l in class_labels:\n",
        "    count_labels[l] += 1\n",
        "\n",
        "# get median count\n",
        "median_freq = np.median(count_labels)\n",
        "\n",
        "# calculate the weigths\n",
        "for i in range(num_classes):\n",
        "    class_weights[i] = median_freq/count_labels[i]\n",
        "\n",
        "# print the weights\n",
        "for i in range(num_classes):\n",
        "    print(\"class\", i, \":\", class_weights[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pPUGMq3NCnP"
      },
      "source": [
        "Now we define the loss function with the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "j-Q72RJ-M84P"
      },
      "outputs": [],
      "source": [
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRBdC31sF6Li"
      },
      "source": [
        "## Define the Optimizer\n",
        "\n",
        "The most common and effective optimizer currently used is **Adam: Adaptive Moments**. You can check out [the paper on it](https://arxiv.org/abs/1412.6980) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pmMjYnCjF6Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fcec54-756a-4afe-84c4-8fcdb866e125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (nonlin1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (nonlin2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (nonlin3): ReLU()\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nonlin4): ReLU()\n",
            "  (fc3): Linear(in_features=84, out_features=7, bias=True)\n",
            "  (nonlin5): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# now lets go back to the initial LeNet architecture\n",
        "net = LeNet()\n",
        "net = net.to(device)\n",
        "\n",
        "# and define an optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oGr-AfqF6Ln"
      },
      "source": [
        "## Network training\n",
        "\n",
        "After everything has been set up, we can now start an actual training on our dataset. To save time, for the moment we will run only ten epochs. Within the training, our dataloader is used to load a batch from our dataset. This batch is forwarded to the model. The corresponding output is compared against its labels with the chosen loss function, here called 'criterion'. Then, the loss values are backpropagated through the whole model.\n",
        "\n",
        "After, the training step a validation step is performed. Here the network is set to .eval() mode in which its weights are not being updated and consequently backprogagation is not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "51fdrcX_vdB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bbb60e-98b2-46b3-88a9-067b4180e953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, training loss: 0.0153, training accuracy: 2.0%, validation loss: 0.0155, validation accuracy: 1.5%, \n",
            "Epoch: 2, training loss: 0.0153, training accuracy: 1.7999999999999998%, validation loss: 0.0155, validation accuracy: 1.4000000000000001%, \n",
            "Epoch: 3, training loss: 0.0153, training accuracy: 1.7999999999999998%, validation loss: 0.0155, validation accuracy: 1.4000000000000001%, \n",
            "Epoch: 4, training loss: 0.0153, training accuracy: 2.1%, validation loss: 0.0155, validation accuracy: 1.4000000000000001%, \n",
            "Epoch: 5, training loss: 0.0153, training accuracy: 2.9000000000000004%, validation loss: 0.0155, validation accuracy: 1.5%, \n",
            "Epoch: 6, training loss: 0.0153, training accuracy: 3.6999999999999997%, validation loss: 0.0155, validation accuracy: 1.5%, \n",
            "Epoch: 7, training loss: 0.0153, training accuracy: 5.1%, validation loss: 0.0155, validation accuracy: 1.7999999999999998%, \n",
            "Epoch: 8, training loss: 0.0153, training accuracy: 5.800000000000001%, validation loss: 0.0155, validation accuracy: 1.7999999999999998%, \n",
            "Epoch: 9, training loss: 0.0153, training accuracy: 6.6000000000000005%, validation loss: 0.0155, validation accuracy: 2.6%, \n",
            "Epoch: 10, training loss: 0.0153, training accuracy: 7.6%, validation loss: 0.0155, validation accuracy: 2.1%, \n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ##### Training loop #####\n",
        "    running_loss_train = 0.0\n",
        "    num_correct_train = 0\n",
        "    num_all_train = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), torch.squeeze(labels).to(device)\n",
        "\n",
        "        # set model to training mode\n",
        "        net.train()\n",
        "\n",
        "        # set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the network output and its loss\n",
        "        outputs = net(inputs[:,0:1,:,:])\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # During training we need to backpropagate the loss and conduct an optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute running loss and accuracy\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        num_correct_train += torch.sum(predicted == labels).item()\n",
        "        num_all_train += labels.size()[0]\n",
        "\n",
        "        training_loss = running_loss_train / num_all_train\n",
        "        training_accuracy = num_correct_train / num_all_train\n",
        "\n",
        "\n",
        "    ##### Validation loop #####\n",
        "    running_loss_val = 0.0\n",
        "    num_correct_val = 0\n",
        "    num_all_val = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), torch.squeeze(labels).to(device)\n",
        "\n",
        "        # set model to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # calculate the network output and its loss\n",
        "        outputs = net(inputs[:,0:1,:,:])\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # compute running loss and accuracy\n",
        "        running_loss_val += loss\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        num_correct_val += torch.sum(predicted == labels).item()\n",
        "        num_all_val += labels.size()[0]\n",
        "\n",
        "        validation_loss = running_loss_val / num_all_val\n",
        "        validation_accuracy = num_correct_val / num_all_val\n",
        "\n",
        "    print('Epoch: {}, training loss: {:.4f}, training accuracy: {}%, validation loss: {:.4f}, validation accuracy: {}%, '.format(epoch+1, training_loss, np.round(training_accuracy, 3) * 100, validation_loss, np.round(validation_accuracy, 3) * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIbJukXZA7ap"
      },
      "source": [
        "## Exercise\n",
        "After performing the training and validation of your system you are now ready to perform the inference on your test set. Implement the inference step for out dataset. Hint: It will look very similar to one the previous loops (train or validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RCAeaw8hvdB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bc0da2-451e-4d85-b16f-f594c1f37ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "Loss for test set is 0.015568957899574033\n",
            "Test accuracy of the network: 397.0%\n"
          ]
        }
      ],
      "source": [
        "running_loss_test = 0.0\n",
        "num_correct_test = 0\n",
        "num_all_test = 0\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "\n",
        "    ########## Implement the data loading and prediction on the test set ##########\n",
        "\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    print(type(labels))\n",
        "    net.eval()  # Set model to validation mode\n",
        "\n",
        "    # -------------------------------\n",
        "    # Calculate the network output and its loss\n",
        "    outputs = net(inputs[:,0:1,:,:])\n",
        "    loss = criterion(outputs, torch.argmax(labels, 1))\n",
        "\n",
        "    ########## ------------------------------- ##########\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    num_correct_test += torch.sum(predicted == labels).item()\n",
        "    num_all_test += labels.size()[0]\n",
        "    running_loss_test += loss.item()\n",
        "\n",
        "    test_loss = running_loss_test / num_all_test\n",
        "    test_accuracy = num_correct_test / num_all_test\n",
        "\n",
        "print('Loss for test set is {}'.format(test_loss))\n",
        "print('Test accuracy of the network: {}%'.format(np.round(test_accuracy, 3)*100))"
      ]
    }
  ]
}